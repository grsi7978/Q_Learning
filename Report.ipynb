{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a8eb7d1",
   "metadata": {},
   "source": [
    "## Mountain Car Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e3a0",
   "metadata": {},
   "source": [
    "For this project I decided to implement what we have learned about reinforcement learning on the Mountain Car environment from the OpenAI Gym/Gymnasium tool. The intent is to develop multiple Q-Learning algorithms and determine if I can get better Rewards from one over the others. \n",
    "\n",
    "The majority of the work will be completed in individual code files listed below in this document.\n",
    "\n",
    "For the development of this project I worked locally on my personal machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d5a64",
   "metadata": {},
   "source": [
    "#### Mountain Car Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ea8b8",
   "metadata": {},
   "source": [
    "The Mountain Car MDP is a deterministic MDP where the car is placed at the bottom of a valley. The goal of the MDP is to accelerate the car to reach the goal state (the flag) at the top of the right hill. There are two versions of the mountain car, one with discrete actions and one with continuous. For the purposes of this project I will be working with the one with discrete actions.\n",
    "\n",
    "The Mountain Car environment is part of the Classic Control environments. Its Action Space = Discrete(3) and it's Observation Space = Box([-1.2 -0.07], [0.6 0.07], (2,), float32).\n",
    "\n",
    "The Action space includes three actions:\n",
    "- O: Push the car to the left\n",
    "- 1: Do nothing\n",
    "- 2: Push the car to the right\n",
    "\n",
    "The Observation Space contains positional and velocity elements:\n",
    "- The first element is the position which can vary between -1.2 and 0.6, representing the car's horizontal position on the track\n",
    "- The second element is the velocity which can vary between -0.07 and 0.07, representing the car's actual velocity\n",
    "\n",
    "This all means that at each step the car must choose one of the three discrete actions and is in a current state given by the continuous vector (Observation Space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5909acaf",
   "metadata": {},
   "source": [
    "More information about the environment can be found here: https://gymnasium.farama.org/environments/classic_control/mountain_car/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ed443",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
