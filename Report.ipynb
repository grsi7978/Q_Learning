{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a8eb7d1",
   "metadata": {},
   "source": [
    "## Mountain Car Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e3a0",
   "metadata": {},
   "source": [
    "For this project I decided to implement what we have learned about reinforcement learning on the Mountain Car environment from the OpenAI Gym/Gymnasium tool (https://gymnasium.farama.org/). \n",
    "\n",
    "The intent is to develop multiple Q-Learning algorithms and determine if I can get better Rewards from one over the others. I will develop a basic Q-Learning algorithm, as well as a Deep Q-Network (DQN).\n",
    "\n",
    "The majority of the work will be completed in individual .py files and compiled in the github repository where this notebook is also located (https://github.com/grsi7978/Q_Learning/tree/main).\n",
    "\n",
    "For the development of this project I worked locally on my personal machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d5a64",
   "metadata": {},
   "source": [
    "### Mountain Car Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ea8b8",
   "metadata": {},
   "source": [
    "The Mountain Car MDP is a deterministic MDP where the car is placed at the bottom of a valley. The goal of the MDP is to accelerate the car to reach the goal state (the flag) at the top of the right hill. There are two versions of the mountain car, one with discrete actions and one with continuous. For the purposes of this project I will be working with the one with discrete actions.\n",
    "\n",
    "The Mountain Car environment is part of the Classic Control environments. Its Action Space = Discrete(3) and it's Observation Space = Box([-1.2 -0.07], [0.6 0.07], (2,), float32).\n",
    "\n",
    "The Action space includes three actions:\n",
    "- 0: Accelerate car to the left\n",
    "- 1: Do nothing\n",
    "- 2: Accelerate car to the right\n",
    "\n",
    "The Observation Space contains positional and velocity elements:\n",
    "- The first element is the position which can vary between -1.2 and 0.6, representing the car's horizontal position on the track\n",
    "- The second element is the velocity which can vary between -0.07 and 0.07, representing the car's actual velocity\n",
    "\n",
    "This all means that at each step the car must choose one of the three discrete actions and is in a current state given by the continuous vector (Observation Space).\n",
    "\n",
    "The base / initial Reward setup for the Mountain Car is a reward of -1 for each timestep that the car is not at the flag (goal).\n",
    "\n",
    "The Mountain Car episode ends in a termination state if the position of the car is greater than or equal to 0.5 (the goal position) OR in a truncation state if teh length the the episode is 200.\n",
    "\n",
    "The above and more information about the environment can be found here: https://gymnasium.farama.org/environments/classic_control/mountain_car/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5909acaf",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c5b7d",
   "metadata": {},
   "source": [
    "Basic Q-Learning implementation: https://github.com/grsi7978/Q_Learning/blob/main/final_project_q_learning.py\n",
    "\n",
    "Basic Q-Learning with adaptive decay implementation: https://github.com/grsi7978/Q_Learning/blob/main/final_project_q_learning_adaptive_decay.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ed443",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30185ec1",
   "metadata": {},
   "source": [
    "DQN implementation: https://github.com/grsi7978/Q_Learning/blob/main/final_project_dqn.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8888fbf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
