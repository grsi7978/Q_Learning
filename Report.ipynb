{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b800b528",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b93cf",
   "metadata": {},
   "source": [
    "For this project I opted to work in two of the game environments, the Mountain Car and Lunar Lander environments. \n",
    "\n",
    "First I developed a Q-Learning algorithm and Deep Q-Network for the Mountain Car environment and then I ported the DQN over and updated it to see how I can implement the same for the Lunar Lander. The breakdown of the environments, the steps I took, and the results can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8eb7d1",
   "metadata": {},
   "source": [
    "## Mountain Car Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e3a0",
   "metadata": {},
   "source": [
    "For this project I decided to implement what we have learned about reinforcement learning on the Mountain Car environment from the OpenAI Gym/Gymnasium tool (https://gymnasium.farama.org/). \n",
    "\n",
    "The intent is to develop multiple Q-Learning algorithms and determine if I can get better Rewards from one over the others. I will develop a basic Q-Learning algorithm, as well as a Deep Q-Network (DQN).\n",
    "\n",
    "The majority of the work will be completed in individual .py files and compiled in the github repository where this notebook is also located (https://github.com/grsi7978/Q_Learning/tree/main).\n",
    "\n",
    "For the development of this project I worked locally on my personal machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d5a64",
   "metadata": {},
   "source": [
    "### Mountain Car Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ea8b8",
   "metadata": {},
   "source": [
    "The Mountain Car MDP is a deterministic MDP where the car is placed at the bottom of a valley. The goal of the MDP is to accelerate the car to reach the goal state (the flag) at the top of the right hill. There are two versions of the mountain car, one with discrete actions and one with continuous. For the purposes of this project I will be working with the one with discrete actions.\n",
    "\n",
    "The Mountain Car environment is part of the Classic Control environments. Its Action Space = Discrete(3) and it's Observation Space = Box([-1.2 -0.07], [0.6 0.07], (2,), float32).\n",
    "\n",
    "The Action space includes three actions:\n",
    "- 0: Accelerate car to the left\n",
    "- 1: Do nothing\n",
    "- 2: Accelerate car to the right\n",
    "\n",
    "The Observation Space contains positional and velocity elements:\n",
    "- The first element is the position which can vary between -1.2 and 0.6, representing the car's horizontal position on the track\n",
    "- The second element is the velocity which can vary between -0.07 and 0.07, representing the car's actual velocity\n",
    "\n",
    "This all means that at each step the car must choose one of the three discrete actions and is in a current state given by the continuous vector (Observation Space).\n",
    "\n",
    "The base / initial Reward setup for the Mountain Car is a reward of -1 for each timestep that the car is not at the flag (goal).\n",
    "\n",
    "The Mountain Car episode ends in a termination state if the position of the car is greater than or equal to 0.5 (the goal position) OR in a truncation state if teh length the the episode is 200.\n",
    "\n",
    "The above and more information about the environment can be found here: https://gymnasium.farama.org/environments/classic_control/mountain_car/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e358039",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5909acaf",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c5b7d",
   "metadata": {},
   "source": [
    "Basic Q-Learning implementation: https://github.com/grsi7978/Q_Learning/blob/main/final_project_q_learning.py\n",
    "\n",
    "Basic Q-Learning with adaptive decay implementation: https://github.com/grsi7978/Q_Learning/blob/main/final_project_q_learning_adaptive_decay.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d404f00",
   "metadata": {},
   "source": [
    "Initially I built a basic Random Agent that made random action decisions just to test out the environment. I left this agent in the code for posterity.\n",
    "\n",
    "I then built a `QAgent` that leveraged Q-Learning to make better decisions and increase the rewards. This QAgent used a learning rate (`alpha`), discount factor (`gamma`), epsilon for exploration (`ep`), and some factors influencing epsilon decay (`ep_decay` and `ep_min`).\n",
    "\n",
    "To complete the `QAgent` I also implemented a few key items:\n",
    "- Q-table\n",
    "    - Indexed by discretized states (via buckets)\n",
    "- Update the Q-values with the Bellman Equation\n",
    "\n",
    "I created an evaluation function `evaluate` to help in development by collecting the total rewards per episode, returning the mean and standard deviation of said rewards, after running the agent for a specified number of episodes (generally had this number set to 20000).\n",
    "\n",
    "I made a function that discretizes the continuous position and velocity states into bucketed indicies called `mapPosVel` to ensure each state maps to a finite grid location in the built Q-table.\n",
    "\n",
    "Then I developed an epsilon-greedy policy function `getAction` that would choose a random exploration action with probability `ep` OR chooses the best exploitation action from the Q-table. In this function I implemented a toggle parameter allowing for the function to always choose exploit if set to True, which was only used when called from the `evaluate` function. The intent here is to test how good the agent is while using a learned policy, not while exploring / taking random actions. \n",
    "\n",
    "The `update` function I created uses the Bellman Equation by getting the current state and next state, computing the next best action using the reward plus the best future Q-value, and applies the temporal difference as an update to the appropriate Q-table entry.\n",
    "\n",
    "The algorithm also decays the exploration over time by simply taking the max between the epsilon minimum and the current epsilon multiplied by the epsilon decay factor.\n",
    "\n",
    "The `qLearning` function applies the agent actions until termination for each episode. It uses some custom reward shaping that rewards the cart for moving faster, being closer to the flag (goal), accelerating in the direction it is currently moving, as well as a very large bonus for reaching the goal state. These rewards were implemented over time and multiple iterations while trying to help the algorithm learn what best actions to take. It is in this function that I also track any successes that occur.\n",
    "\n",
    "This implementation required a lot of parameter tuning. You can see just some of the tuning I did in the code comments as I began recording already tested values after it became too difficult to remember all the iterations that I attempted. For instance, for the buckets I was using for the discretized states I iterated through many different sizes ((40,30), (24,18), (18,14), (20,15)) and some others that were not recorded before finding the best combination to be (20,15). Other basic parameters that were tweaked many times included the epsilon decay (`ep_decay`) and epsilon minimum (`ep_min`).\n",
    "\n",
    "In an attempt to achieve better Reward scores I also implemented a nearly identical version of the Q-Learning implementation, with the addition of an adaptive decay. In this version the epsilon decay rate is increase if the reward is improving beyond a threshold set (close to the goal threshold) or declines (slower decay) if the reward is not moving beyond said threshold. The intent here was to help prevent early convergence. \n",
    "\n",
    "I also added a function `plotRewards` that simply plotted the learning progress and success rate of the algorithm. See below for an example output of the graph. This function helped me when tuning the parameters mentioned above as I could compare graphs and see how fast or consistently the learning process was actualizing. \n",
    "\n",
    "![Figure_1](img/Q_Learning_Figure_1.png)\n",
    "\n",
    "I also had the learning function outputting print statements that included the episode number, the total reward for the episode, and the current epsilon value. To make the output managable I restricted the output to every 100 episodes as I was generally running for > 15000 episode. The Final Evaluation over 10 episodes' Average Reward and standard deviation, as well as the total number of successful runs were also output. This another way I could monitor the trends and see if there was any improvement while I was tweaking the parameters and the reward scaling.\n",
    "\n",
    "![Figure_2](img/Output_Figure_1.png)\n",
    "\n",
    "Seeing a positive trend in the success rate was the best true measure that the algorithm was working as intended. It appears that a Reward score of < ~-130 would generally lead to a successful run, but manually tracking the actual successes was the only real way I was able to guarantee that a run completed with the cart reaching the flag. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ef885",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf61671",
   "metadata": {},
   "source": [
    "Overall the adaptive decay Q-Learning method had ~100% success rate with a somewhat low Reward score (~140-170) while the non-adaptive Q-Learning method ranged from 60%-90% success rates generally but with better average scores. This indicates that when the non-adaptive method was successful it was successful noticably faster than the adaptive decay method, but the adaptive decay method was consistently successful while the non-adaptive was not.\n",
    "\n",
    "Here is a gif of ten runs made by the adaptive decay Q-Learning method:\n",
    "\n",
    "![SegmentLocal](img/adaptive_q_learning.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ed443",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30185ec1",
   "metadata": {},
   "source": [
    "DQN implementation: https://github.com/grsi7978/Q_Learning/blob/main/final_project_dqn.py \n",
    "\n",
    "For the DQN I leveraged a few key concepts:\n",
    "- Q-Network (policy net) and Target Network: These are used to stabalize the learning\n",
    "    - I restricted updates to the Q-Network until I had enough samples to make it worth while (explained again in more detail below)\n",
    "    - I periodically updated the Target Network to match the Q-Network: The idea here is to make the targets used in learning more stable\n",
    "    - Here is where I am setting this up, which I mainly got from this <a href=\"https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\">tutorial</a>:\n",
    "    ```\n",
    "        self.q_net = DQN(input_dim, self.n_actions).to(self.device)\n",
    "        self.target_net = DQN(input_dim, self.n_actions).to(self.device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "    ```\n",
    "    - Here is where I periodically update the target network which I mainly got from this <a href=\" https://lightning.ai/docs/pytorch/2.0.9/notebooks/lightning_examples/reinforce-learning-DQN.html\">tutorial</a>:\n",
    "    ```\n",
    "        if self.step_count % self.target_update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "    ```\n",
    "- Double DQN: Used to reduce overestimation\n",
    "    - Uses `q_net` to choose best action in the next state\n",
    "    - Uses `target_net` to evaluate the value of that action\n",
    "    - Here is where I am setting this up:\n",
    "    ```\n",
    "        next_actions = self.q_net(next_states).argmax(1, keepdim=True)  # action selection\n",
    "        max_next_q = self.target_net(next_states).gather(1, next_actions)  # action evaluation\n",
    "    ```\n",
    "- Replay Buffer: Stores transitions in memory and then samples them randomly for training purposes\n",
    "    - Here are the non-sequential calls I make to implement this, which I mainly got from this <a href=\" https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\">tutorial</a>:\n",
    "        - `self.memory = deque(maxlen=mem_size)`\n",
    "        - `self.memory.append((state, action, reward, next_state, done))`\n",
    "        - `batch = random.sample(self.memory, self.batch_size)`\n",
    "- Epsilon Greedy Policy: Used in each training episode to collect transitions\n",
    "    - Gradually will shift from exploration to exploitation\n",
    "    - This is basically the same as the basic Q-Learning implementation\n",
    "- Normalization of Inputs: Used to standardize state values into similar scales as neural networks learn faster and more effictively with feature inputs on similar scales\n",
    "    - Here is one place I am using this:\n",
    "    ```\n",
    "        state_mean = np.array([-0.3, 0.0], dtype=np.float32) # -0.3 is center of -1.2,0.6 (position) and 0.0 is center of -0.07,0.07 (velocity)\n",
    "        state_std = np.array([0.9,0.07], dtype=np.float32) # 0.9 = (0.6 - -1.2) / 2\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8888fbf",
   "metadata": {},
   "source": [
    "![Figure_3](img/Output_Figure_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd8b8c",
   "metadata": {},
   "source": [
    "Initially I did not have reward bonuses for certain states / action combinations like I eventually did in the Q-Learning models. Meaning I was not rewarding for being closer to the goal state, for moving in the right direction, etc. I was not sure that these would matter as much in a DQN. However, after many failed attempts at getting satistifying results I did end up adding some of these, namely a `velocity_bonus`, `position_reward`, and large early termination reward (successful termination):\n",
    "\n",
    "```\n",
    "            velocity_bonus = 0.1 * abs(next_state[1])\n",
    "            position_min = env.observation_space.low[0]\n",
    "            position_max = env.observation_space.high[0] \n",
    "            position_reward = 0.3 * ((next_state[0] - position_min) / (position_max - position_min))\n",
    "\n",
    "            new_reward = reward + velocity_bonus + position_reward\n",
    "```\n",
    "I played around with some different values for these, such as adding 10.0 for early successful termination initially, but eventually bumped that up to 100.0. \n",
    "\n",
    "There were many other values that I toyed with, trying to massage the DQN so that it would perform better. Some such values included the `episodes` count used for training (tried 1000, 1500, landed on 2000), the learning rate (`lr`) of the model (tried 1e-3, 1e-4, 3e-4), and the target update frequencey (`target_update_freq`) of the model (started at 5000 landed on 1000). There were many other values that I spent time tweaking up or down depending on what the variable was being used for but eventually I seemed to have found a manageable balance for the model where I was getting 10/10 successes on every run.\n",
    "\n",
    "One of the most important changes to a value set that I made was an increase to the number of neurons in the network model. I initially had the DQN model with `56` units per layer but bumped it up to `128` per. The intent here was to help the model approximate better.\n",
    "\n",
    "One of the other many changes I made during implementation was to normalize the velocity and position features to help the neural net better use the state values and use both features more equally. Velocity spans a much smaller range / is on a smaller scale (-0.07 to 0.07 with a range of 0.14) than Position (-1.2 to 0.6 with a range of 1.8) meaning that both were likely not influencing the learning nearly as evenly as I would like.\n",
    "\n",
    "Initially I had code that looked like this:\n",
    "```\n",
    "next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "```\n",
    "But I normalized it so it looked like this:\n",
    "```\n",
    "next_states = (torch.FloatTensor(next_states).to(self.device) - state_mean) / state_std\n",
    "```\n",
    "\n",
    "Another change I made during implementation was to add a warm up period to the agent in the form of delaying updating of said agent for a certain length of time (1000 steps). I accomplished this simply by adding an additional `warmup_steps` variable to the agent and comparing the steps count to the number of experiences in the replay buffer `self.memory`. If the number of experiences was less than the `warmup_steps` variable value, or the `batch_size` variable value, then no update would occur. I already had the `batch_size` limitation in place to ensure there was aenough datat for one mini-batch, but the `warmup_steps` limitation ensures that there is a diverse set of transitions in the buffer. Through some research early on when the DQN was performing terribly I saw that early episodes are often considered poor to use as they may have sparse rewards and everything is still random. This implementation allowed for more valuable transistions to be included in the batches.\n",
    "\n",
    "One final change I will list here, though there were many many more that occured during development, is the move from the `MSELoss` function to the `SmoothL1Loss` loss function. Looking online, RL targets are often noisey and rarely clean. The `SmoothL1Loss` combines MSE and MAE to handle both small and large errors VS the `MSELoss` function which generally only handles small errors and outliers well. This update made a noticable change to the output of the DQN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c54325",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a4e739",
   "metadata": {},
   "source": [
    "Below you can see the improvement that the DQN experienced during training over time.\n",
    "\n",
    "![Figure_4](img/DQN_Learning_Figure_1.png)\n",
    "\n",
    "Overall the DQN ended up having a nearly 100% success rate in all the times that I ran it in its final state. This is as complete and effective as the adaptive decay Q-Learning.\n",
    "\n",
    "![Figure_5](img/DQN_Success_Rate_Figure_1.png)\n",
    "\n",
    "Finally here is a gif of 10 runs of the DQN implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5182f2bd",
   "metadata": {},
   "source": [
    "![SegmentLocal](img/dqn_learning.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1208c",
   "metadata": {},
   "source": [
    "#### Q-Learning VS DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828d94c",
   "metadata": {},
   "source": [
    "Though both the adaptive decay Q-Learning and DQN generally experienced ~100% success rates over their 10 evaluation episodes in each of the runs I did they did not **always** experience this. The DQN did seem to have slightly better performance, rarely falling below the 100% success rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61155cf",
   "metadata": {},
   "source": [
    "## Lunar Lander Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c17f9",
   "metadata": {},
   "source": [
    "### Lunar Lander Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dd6656",
   "metadata": {},
   "source": [
    "The Lunar Lander environment is a rocket trajectory optimization problem. There are two versions discrete or continuous. The goal is to try to have the ship land (not crash land) on the landing pad which is always at the coordinates (0,0). It is important to note that fuel is infinite for the ship, this allows for learning before landing without fear of a time limit causing a crash landing.\n",
    "\n",
    "The Action Space includes 4 Discrete Actions:\n",
    "- 0: Do nothing\n",
    "- 1: Fire left orientation engine\n",
    "- 2: Fire main enging (center bottom)\n",
    "- 3: Fire right orientation engine\n",
    "\n",
    "The Observation Space is an 8-dimensional vector:\n",
    "- 0: x-position of lander\n",
    "- 1: y-position of lander\n",
    "- 2: x-velocity of lander\n",
    "- 3: y-velocity of lander\n",
    "- 4: angle of the lander in radians\n",
    "- 5: angular velocity of the lander (speed at which the angle is changing)\n",
    "- 6: left leg contact boolean (represents whether or not the leg is in contact with the ground)\n",
    "- 7: right leg contact boolean (represents whether or not the leg is in contact with the ground)\n",
    "\n",
    "The base / initial Reward setup for the Lunar Lander has the total reward of an episode being the sum of the rewards for all the steps within the episode. Elements that impact the overall reward are:\n",
    "- closer (increased reward)/further (decreased reward) the lander is to the landing pad\n",
    "- slower(increased reward)/faster(decreased reward) the lander is moving\n",
    "- decreased the more the lander is tilted (angle not horizontal)\n",
    "- increased by 10 points for each leg that is in contact with the ground\n",
    "- decreased by 0.03 points each frame a side engine is firing\n",
    "- decreased by 0.3 points each frame the main engine is firing\n",
    "- decreased by 100 for crashing\n",
    "- increased by 100 for landing safely\n",
    "\n",
    "It is important to note that an episode is considered a solution if it scores at least 200 points. **Note**: This is a noticably different set up than the Mountain Car as I was manually tweaking the rewards in the Mountain Car algorithms to influence its behavior, and a reaching the goal distance/location (flag) was considered a success. Due to this set up for the Lunar Lander I will not be shaping rewards as the algorithm progresses to ensure desired readability and accuracy in the results.\n",
    "\n",
    "The above and more information about the Lunar Lander environment can be found here: https://gymnasium.farama.org/environments/box2d/lunar_lander/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a12d8b",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65107ac8",
   "metadata": {},
   "source": [
    "After some research I opted to use the Lunar Lander in the discrete form. This allowed for use of the DQN model that I had been building and learning about in class. If I shifted to the continuous form then the ideal implementations would have been something along the lines of a DDPG or SAC instead of a DQN. Even if I included policy gradient extensions it seems as though a DQN is not ideal for the Lunar Lander continuous model. This is because the DQN still uses the highest Q-values when determining each action to take, and in a continuous space, to my understanding it is very difficult to determine the Q-value for every possible action since there are so many possibilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74e94d8",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e5c49",
   "metadata": {},
   "source": [
    "![Figure_6](img/dqn_lunar_lander_reward_time_01.png)\n",
    "\n",
    "Below is a gif of ten evaluation runs of the Lunar Lander game:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b5485",
   "metadata": {},
   "source": [
    "![SegmentLocal](img/dqn_learning_lunar_lander.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e85bc",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d7630",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dcfb1f",
   "metadata": {},
   "source": [
    "- DQN\n",
    "    - https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "    - https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial \n",
    "    - https://www.slingacademy.com/article/implementing-deep-q-networks-dqn-in-pytorch-for-complex-environments/\n",
    "    - https://medium.com/@hkabhi916/mastering-deep-q-learning-with-pytorch-a-comprehensive-guide-a7e690d644fc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa32e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
